具身人工智能（Embodied AI）对于实现通用人工智能（AGI）至关重要，并作为连接网络空间和物理世界的各种应用的基础。最近，多模态大型模型（MLMs）和世界模型（WMs）的出现因其卓越的感知、交互和推理能力而受到显著关注，使它们成为具身智能体大脑的有希望的架构。然而，目前还没有针对MLMs时代的具身AI的全面调查。在这项调查中，我们对具身AI的最新进展进行了全面探索。我们的分析首先通过具身机器人和模拟器的前沿代表性作品，全面了解研究重点及其局限性。然后，我们分析了四个主要研究目标：1）具身感知，2）具身交互，3）具身智能体，以及4）仿真到现实的适应性，涵盖了最先进的方法、基本范式和综合数据集。此外，我们探讨了虚拟和现实具身智能体中MLMs的复杂性，强调了它们在促进动态数字和物理环境中的交互中的重要性。最后，我们总结了具身AI的挑战和局限性，并讨论了它们潜在的未来方向。我们希望这项调查能为研究社区提供基础参考，并激发持续的创新。  

我们翻译解读最新论文：关于嵌入式人工智能的全面调查，文末有论文链接。![图片](https://mmbiz.qpic.cn/mmbiz_jpg/Eddk7AtcliaMmMJEoRmtw7FdZaGqdvz7lvADibR7JHibmGmbJP0zdF2bUZYwzmibmibyjomF2Wg0scOicCkCfRLYe26A/640?wx_fmt=other&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1&tp=webp)![图片](https://mmbiz.qpic.cn/mmbiz_png/Eddk7AtcliaMYpR1BLiaNVbDd0hRefSskPLbiaiaibevAdEIauicSAIbGhlv5TEUv0bicwpBbNjJq0SXsG3vG3BQerCDw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)作者：张长旺，图源：旺知识

关键词： 具身AI，网络空间，物理世界，多模态大型模型，世界模型，智能体，机器人学

I. 引言

具身AI最初由艾伦·图灵在1950年提出的图灵测试提出\[1\]，旨在确定智能体是否能够展示不仅仅是在虚拟环境中解决抽象问题（网络空间1），而且也能够导航物理世界的复杂性和不可预测性。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)图1 展示了谷歌学术上关于具身AI主题的搜索结果。垂直轴和水平轴分别表示出版物数量和年份。自2023年MLMs的突破以来，出版物呈指数级增长。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

网络空间中的智能体通常被称为无体AI，而物理空间中的智能体则是具身AI（表I）。最近在多模态大型模型（MLMs）方面的进步，为具身模型注入了强大的感知、交互和规划能力，以开发通用的具身智能体和机器人，它们积极地与虚拟和物理环境进行交互\[2\]。因此，具身智能体被广泛认为是MLMs的最佳载体。最近的代表性具身模型是RT-2 \[3\]和RT-H \[4\]。然而，当前MLMs在长期记忆能力、理解复杂意图以及复杂任务分解方面的能力是有限的。为了实现通用人工智能（AGI），具身AI的发展是一条基本途径。与像ChatGPT \[5\]这样的对话智能体不同，具身AI认为通过控制物理体现并与模拟和物理环境进行交互，可以实现真正的AGI \[6\]–\[8\]。随着我们站在AGI驱动创新的前沿，深入探索具身AI领域，解开它们的复杂性，评估它们当前的发展阶段，并思考它们未来可能遵循的潜在轨迹至关重要。如今，具身AI包含了计算机视觉（CV）、自然语言处理（NLP）和机器人技术等各种关键技术，最具代表性的是具身感知、具身交互、具身智能体和仿真到现实机器人控制。因此，通过全面调查捕捉具身AI的不断发展的景观，以追求AGI是至关重要的。具身智能体是具身AI最突出的基础。对于一个具身任务，具身智能体必须充分理解语言指令中的人类意图，积极探索周围环境，全面感知来自虚拟和物理环境的多模态元素，并为复杂任务执行适当的行动\[12\]，\[13\]，如图2所示。多模态模型的快速发展在复杂环境中展示了优越的多功能性、灵活性和泛化能力，与传统的深度强化学习方法相比。最先进的视觉编码器\[14\]，\[15\]提供的预训练视觉表示提供精确的对象类别、姿态和几何形状估计，使具身模型能够彻底感知复杂和动态的环境。功能强大的大型语言模型（LLMs）使机器人更好地理解来自人类的语言指令。有希望的MLMs为对齐来自具身机器人的视觉和语言表示提供了可行的方法。世界模型\[16\]，\[17\]展示了显著的模拟能力和对物理法则的理解，使具身模型能够全面理解物理和真实环境。这些创新使具身智能体能够全面感知复杂环境，自然地与人类交互，并可靠地执行任务。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

具身AI的进步已经取得了快速发展，在研究社区中引起了显著关注（图1），并被认为是实现AGI的最可行路径。谷歌学术报告称，具身AI出版物的数量很大，仅在2023年就有大约10700篇论文发表。这相当于平均每天29篇论文或每小时超过一篇论文。尽管对从MLMs中获取强大的感知和推理能力非常感兴趣，但研究社区缺乏一个全面的调查，可以帮助整理现有的具身AI研究、面临的挑战以及未来的研究方向。在MLMs时代，我们的目标是通过对网络空间到物理世界的具身AI进行全面系统调查来填补这一空白。我们从不同的角度进行调查，包括具身机器人、模拟器、四个代表性的具身任务（视觉主动感知、具身交互、多模态智能体和仿真到现实的机器人控制）以及未来的研究方向。我们相信这项调查将提供一个清晰的大局图，展示我们已经取得的成就，以及我们可以沿着这一新兴但非常有前景的研究方向进一步取得的成就。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

与以往工作的不同之处：尽管已经有几篇关于具身AI的调查论文\[6\]，\[18\]–\[20\]，但它们大多已经过时，因为它们是在2023年左右的MLMs时代之前发表的。据我们所知，2023年之后只有一篇调查论文\[8\]，它只关注了视觉-语言-动作具身AI模型。然而，MLMs、WMs和具身智能体并没有被完全考虑。此外，最近的具身机器人和模拟器的发展也被忽视了。为了解决这一快速发展领域中全面调查论文的缺乏，我们提出了这项全面调查，涵盖了代表性的具身机器人、模拟器和四个主要研究任务：具身感知、具身交互、具身智能体和仿真到现实的机器人控制。

总之，这项工作的主要贡献有三个方面。首先，它对具身AI进行了系统回顾，包括具身机器人、模拟器和四个主要研究任务：视觉主动感知、具身交互、具身智能体和仿真到现实的机器人控制。据我们所知，这是第一次从网络和物理空间对齐的角度，基于MLMs和WMs，对具身AI进行全面调查，提供了一个广泛的概述，并对现有研究进行了彻底的总结和分类。其次，它检查了具身AI的最新进展，为跨多个模拟器和数据集的当前工作提供了全面的基准测试和讨论。第三，它确定了未来研究的几个研究挑战和潜在方向，以实现具身AI的AGI。本文的其余部分组织如下。第2节介绍各种具身机器人。第3节描述通用和真实场景的模拟器。第4节介绍具身感知，包括主动视觉感知、3D视觉定位、视觉语言导航和非视觉感知。第5节介绍具身交互。第6节介绍具身智能体，包括具身多模态基础模型和具身任务规划。第7节介绍仿真到现实的适应性，包括具身世界模型、数据收集和训练以及具身控制。第8节，我们讨论有希望的研究方向。

II. 具身机器人

具身智能体积极与物理环境互动，包括广泛的具身形式，包括机器人、智能家电、智能眼镜、自动驾驶汽车等。其中，机器人作为最突出的具身形式之一。根据应用的不同，机器人被设计成各种形态，利用其硬件特性来执行特定任务，如图4所示。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

**A. 固定基机器人**

如图4(a)所示，固定基机器人广泛应用于实验室自动化、教育培训和工业制造，因为它们体积小巧且操作精度高。这些机器人具有坚固的基座和结构，确保了操作期间的稳定性和高精度。它们配备了高精度传感器和执行器，能够实现微米级精度，使它们适合需要高精度和重复性的任务\[21\]。此外，固定基机器人具有高度的可编程性，允许用户适应各种任务场景，例如Franka（Franka Emika panda）\[22\]、Kuka iiwa（KUKA）\[23\]和Sawyer（Rethink Robotics）\[24\]。然而，固定基机器人也有一定的缺点。它们的固定基座设计限制了它们的操作范围和灵活性，阻碍了它们在大面积内的移动或调整位置，导致它们与人类和其他机器人的协作受到限制\[21\]。

**B. 轮式机器人和履带式机器人**

对于移动机器人，它们可以面对更复杂和多样化的应用场景。如图4(b)所示，轮式机器人以其高效的机动性而闻名，在物流、仓储和安全检查中得到广泛应用。轮式机器人的优点包括结构简单、成本相对较低、高能效和在平坦表面上快速移动的能力\[21\]。这些机器人通常配备有高精度传感器，如激光雷达和摄像头，使其能够自主导航和环境感知，使它们在自动化仓库管理和检查任务中非常有效，例如Kiva机器人（Kiva Systems）\[25\]和Jackal机器人（Clearpath Robotics）\[26\]。然而，轮式机器人在复杂地形和恶劣环境中的机动性有限，特别是在不平坦的地面上。此外，它们的载重能力和机动性也受到一定限制。与此不同，履带式机器人具有强大的越野能力和机动性，在农业、建筑和灾难恢复中显示出潜力，如图4(c)所示。履带系统提供了更大的地面接触面积，分散了机器人的重量，降低了在泥泞和沙滩等软质地形上沉陷的风险。此外，履带式机器人配备了强大的动力和悬挂系统，以在复杂地形上保持稳定性和牵引力\[27\]。因此，履带式机器人也用于军事等敏感领域。iRobot的PackBot是一种多才多艺的军事履带机器人，能够执行侦察、爆炸物处理和救援任务\[28\]。然而，由于履带系统的高摩擦，履带式机器人通常遭受低能效的困扰。此外，它们在平坦表面上的移动速度比轮式机器人慢，以及它们的灵活性和机动性也受到限制。

**C. 四足机器人**

四足机器人以其稳定性和适应性而闻名，非常适合复杂地形探索、救援任务和军事应用。受四足动物启发，这些机器人能够在不平坦的表面上保持平衡和机动性，如图4(d)所示。多关节设计使它们能够模仿生物运动，实现复杂的步态和姿势调整。高可调性使机器人能够自动调整其姿态以适应不断变化的地形，增强机动性和稳定性。传感系统，如激光雷达和摄像头，提供环境感知，使机器人能够自主导航并避免障碍\[29\]。几种类型的四足机器人被广泛使用：Unitree Robotics、Boston Dynamics Spot和ANYmal C。Unitree Robotics的Unitree A1和Go1以其成本效益和灵活性而闻名。A1\[30\]和Go1\[31\]具有强大的机动性和智能避障能力，适用于各种应用。Boston Dynamics的Spot以其卓越的稳定性和操作灵活性而闻名，通常用于工业检查和救援任务。它具有强大的负载能力和适应性，能够在恶劣环境中执行复杂任务\[32\]。ANYbotics的ANYmal C以其模块化设计和高耐用性而广泛用于工业检查和维护。ANYmal C配备了自主导航和远程操作能力，适合长时间的户外任务甚至极端的月球任务\[33\]。四足机器人的复杂设计和高制造成本导致了大量初始投资，限制了它们在成本敏感领域的使用。此外，它们在复杂环境中的电池续航能力有限，需要频繁充电或更换电池以进行长时间操作\[34\]。

**D. 仿人机器人**

仿人机器人以其类人形态而著称，在服务行业、医疗保健和协作环境中越来越普遍。这些机器人可以模仿人类的动作和行为模式，提供个性化服务和支持。它们灵巧的手设计使它们能够执行复杂和复杂的任务，与其他类型的机器人区分开来，如图4(e)所示。这些手通常具有多个自由度和高精度传感器，使它们能够模仿人类的抓握和操纵能力，在医疗手术和精密制造等领域尤为重要\[35\]。在当前的仿人机器人中，Atlas（Boston Dynamics）以其卓越的机动性和稳定性而闻名。Atlas可以执行跑步、跳跃和滚动等复杂动态动作，展示了仿人机器人在高度动态环境中的潜力\[36\]。HRP系列（AIST）在各种研究和工业应用中得到应用，设计重点是高稳定性和灵活性，使其在复杂环境中特别有效，尤其是与人类的协作任务\[37\]。ASIMO（Honda）是最知名的仿人机器人之一，能够行走、跑步、爬楼梯，以及识别面部和手势，使其适合接待和导游服务\[38\]。此外，一种小型社交机器人Pepper（Softbank Robotics）能够识别情绪并进行自然语言交流，在客户服务和教育环境中得到广泛应用\[39\]。然而，仿人机器人在保持复杂环境中的操作稳定性和可靠性方面面临挑战，由于其复杂的控制系统，包括健壮的双足行走控制和灵巧的手部抓握\[40\]。此外，基于液压系统的传统仿人机器人以其庞大的结构和高昂的维护成本为特点，正逐渐被电机驱动系统所取代。最近，特斯拉和Unitree Robotics推出了基于电机系统的仿人机器人。通过整合LLMs，仿人机器人预计将智能地处理各种复杂任务，填补制造业、医疗保健和服务行业中的劳动力缺口，从而提高效率和安全性\[41\]。

**E. 生物模拟机器人**

不同地，生物模拟机器人通过模拟自然生物体的高效运动和功能，在复杂和动态的环境中执行任务。通过模仿生物体的形式和运动机制，这些机器人在医疗保健、环境监测和生物研究等领域展示了巨大的潜力\[21\]。通常，它们使用柔性材料和结构来实现逼真、敏捷的运动，并最大限度地减少对环境的影响。重要的是，生物模拟设计可以通过模仿生物体的高效运动机制显著提高机器人的能效，使它们在能源消耗方面更加经济\[42\]，\[43\]。这些生物模拟机器人包括鱼形机器人\[44\]，\[45\]，昆虫形机器人\[46\]，\[47\]和软体机器人\[48\]，如图4(f)所示。然而，生物模拟机器人面临几个挑战。首先，它们的设计和制造过程复杂且成本高，限制了大规模生产和广泛应用。其次，由于它们使用柔性材料和复杂的运动机制，生物模拟机器人在极端环境中的耐用性和可靠性受到限制。

III. 具身模拟器

具身模拟器对具身人工智能至关重要，因为它们提供了成本效益高的实验，通过模拟潜在的危险场景来确保安全，具有在不同环境中测试的可扩展性、快速原型制作能力、更广泛的研究社区的可访问性、控制环境进行精确研究、用于训练和评估的数据生成，以及用于算法比较的标准基准。为了使智能体能够与环境互动，有必要构建一个现实模拟的环境。这需要考虑环境的物理特性、对象的属性以及它们之间的互动。本节将介绍两部分常用的模拟平台：基于底层模拟的通用模拟器和基于真实场景的模拟器。

**A. 通用模拟器**

真实环境中的物理互动和动态变化是不可替代的。然而，在物理世界中部署具身模型通常会带来高昂的成本和面临众多挑战。通用模拟器提供了一个与物理世界紧密相似的虚拟环境，允许进行算法开发和模型训练，这在成本、时间和安全性方面提供了显著的优势。Isaac Sim \[49\] 是一个先进的机器人和人工智能研究的模拟平台。它具有高保真度的物理模拟、实时光线追踪、广泛的机器人模型库和深度学习支持。它的应用场景包括自动驾驶、工业自动化和人机交互。Gazebo \[60\] 是一个用于机器人研究的开源模拟器。它拥有丰富的机器人库，并与ROS紧密集成。它支持各种传感器的模拟，并提供众多预建的机器人模型和环境。它主要用于机器人导航和控制以及多机器人系统。PyBullet \[52\] 是Bullet物理引擎的Python接口。它易于使用，具有多样化的传感器模拟和深度学习集成。PyBullet支持实时物理模拟，包括刚体动力学、碰撞检测和约束求解。表II展示了10个通用模拟器的关键特性和主要应用场景。它们在具身人工智能领域各自提供独特的优势。研究人员可以根据具体的研究需求选择最合适的模拟器，从而加速具身人工智能技术的发展和应用。图5显示了通用模拟器的可视化效果。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

**B. 基于真实场景的模拟器**

实现在家庭活动中的通用具身智能体一直是具身人工智能研究的主要焦点。这些具身智能体需要深入理解人类日常生活，并执行复杂的具身任务，如室内环境中的导航和交互。为了满足这些复杂任务的需求，模拟环境需要尽可能接近真实世界，这对模拟器的复杂性和现实性提出了高要求。这导致了基于真实世界环境的模拟器的创建。这些模拟器大多从真实世界收集数据，创建照片般逼真的3D资产，并使用像UE5和Unity这样的3D游戏引擎构建场景。丰富和真实的场景使基于真实世界环境的模拟器成为家庭活动中具身人工智能研究的首选。

AI2-THOR \[61\] 是由Allen人工智能研究所领导的基于Unity3D的室内具身场景模拟器。作为一个在真实世界中构建的高保真模拟器，AI2-THOR拥有丰富的交互场景对象，并且为它们分配了物理属性（如打开/关闭甚至冷/热）。AI2-THOR由两部分组成：iTHOR和RoboTHOR。iTHOR包含120个房间，分为厨房、卧室、浴室和客厅，拥有2000多个独特的交互对象，并支持多智能体模拟；

RoboTHOR包含89个模块化公寓，这些公寓与真实世界中的真实场景相对应。到目前为止，已经有超过一百篇基于AI2-THOR的作品发表。Matterport 3D \[62\] 在R2R \[63\]中提出，通常用作大规模2D-3D视觉数据集。Matterport3D数据集包括90个建筑室内场景，包含10800个全景图和194400个RGB-D图像，并提供表面重建、相机姿态和2D和3D语义分割注释。

Matterport3D将3D场景转换为离散的“视点”，具身智能体在Matterport3D场景中的相邻“视点”之间移动。在每个“视点”，具身智能体可以获得一个1280x1024全景图像（18× RGB-D）。Matterport3D是最重要的具身导航基准之一。

Virtualhome \[64\] 是Puig等人带来的家庭活动具身AI模拟器。Virtualhome最特别的是其环境由环境图表示。环境图表示场景中的对象及其相关关系。用户也可以自定义和修改环境图以实现场景对象的自定义配置。这种环境图为具身智能体理解环境提供了一种新方式。与AI2-THOR类似，Virtualhome还提供了大量的交互对象，具身智能体可以与它们互动并改变它们的状态。Virtualhome的另一个特点是其简单易用的API。具身智能体的操作简化为“操作+对象”的格式。这一特点使Virtualhome在具身规划、指令分解等研究领域得到广泛应用。

Habitat \[65\] 是Meta推出的用于大规模人机交互的开源模拟器。基于Bullet物理引擎，Habitat实现了高性能、高速、并行3D模拟，并为具身智能体的强化学习提供了丰富的接口。Habitat具有极高的开放度。研究人员可以在Habitat中导入和创建3D场景，或者使用Habitat平台上丰富的开放资源进行扩展。Habitat有许多可定制的传感器，并支持多智能体模拟。来自开放资源或自定义的多个具身智能体（例如，人类和机器狗）可以在模拟器中合作，自由移动，并与场景进行简单交互。因此，Habitat正在吸引越来越多的关注。

与其它更关注场景的模拟器不同，SAPIEN \[66\] 更加关注模拟对象之间的交互。基于PhysX物理引擎，SAPIEN提供了细粒度的具身控制，可以通过ROS接口通过力和扭矩实现基于关节的控制。基于PartNet-Mobility数据集，SAPIEN提供了室内模拟场景，包含丰富的交互对象，并支持自定义资源的导入。与像AI2-THOR这样的模拟器不同，后者直接改变对象的状态，SAPIEN支持模拟物理交互，具身智能体可以通过物理动作控制对象的铰接部分，从而改变对象的状态。这些功能使SAPIEN非常适合训练具身AI的细粒度对象操作。

iGibson \[67\] \[68\] 是斯坦福推出的开源模拟器。构建在Bullet物理引擎上，iGibson提供了15个高质量的室内场景，并支持从其他数据集（如Gibson和Matterport3D）导入资产。作为一个面向对象的模拟器，iGibson为对象分配了丰富的可变属性，不仅限于对象的运动属性（姿态、速度、加速度等），还包括温度、湿度、清洁度、开关状态等。此外，除了其他模拟器中的标准深度和语义传感器外，iGibson还为具身智能体提供了激光雷达，使智能体能够轻松获取场景中的3D点云。关于具身智能体配置，iGibson支持连续动作控制和细粒度关节控制。这允许iGibson中的具身智能体在移动时与对象进行精细交互。

TDW \[69\] 由MIT推出。作为最新的具身模拟器之一，TDW结合了高保真视频和音频渲染、逼真的物理效果和单一灵活的控制器，在模拟环境的感知和交互方面取得了一定的进展。TDW将多个物理引擎集成到一个框架中，可以实现各种材料（如刚体、软体、织物和流体）的物理交互模拟，并在与对象交互时提供情境声音。因此，TDW与其他模拟器相比迈出了重要的一步。TDW支持部署多个智能代理，并为用户提供了丰富的API库和资产库，允许用户根据需要自由定制场景和任务，甚至是户外场景和相关任务。表III总结了上述所有基于真实场景的模拟器。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

Sapien以其设计脱颖而出，专门用于模拟与门、橱柜和抽屉等关节对象的交互。VirtualHome以其独特的环境图而著称，这有助于基于自然语言描述的环境的高级具身规划。虽然AI2Thor提供了丰富的交互场景，但这些交互与VirtualHome中的交互类似，都是基于脚本的，缺乏真实的物理交互。这种设计足以满足不需要细粒度交互的具身任务。iGibson和TDW都提供了细粒度的具身控制和高度模拟的物理交互。

IV. 具身感知

未来的具身感知的“北斗星”是具身中心的视觉推理和社会智能\[74\]。与传统的图像中的对象识别不同，具有具身感知的智能体必须在物理世界中移动并与环境互动。这要求对3D空间和动态环境有更深入的理解。具身感知需要视觉感知和推理，理解场景中的3D关系，并基于视觉信息预测和执行复杂任务。

**A. 主动视觉感知**

主动视觉感知系统需要基本能力，如状态估计、场景感知和环境探索。如图7所示，这些能力已在视觉同时定位与地图构建（vSLAM）、3D场景理解（3D Scene Understanding）和主动探索（Active Exploration）等领域进行了广泛研究\[118\]，\[119\]。这些研究领域有助于开发鲁棒的主动视觉感知系统，促进在复杂、动态环境中改进环境交互和导航。我们简要介绍这三个组成部分，并在表IV中总结了每个部分中提到的方法。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

图7 展示了主动视觉感知的示意图。视觉SLAM和3D场景理解为被动视觉感知提供了基础，而主动探索可以为被动感知系统提供主动性。这三个要素相辅相成，对主动视觉感知系统至关重要。

1.  **视觉同时定位与地图构建**（Visual Simultaneous Localization and Mapping, vSLAM）：同时定位与地图构建（Simultaneous Localization and Mapping, SLAM）是一种技术，它可以在未知环境中确定移动机器人的位置，同时构建该环境的地图\[121\]，\[122\]。基于范围的SLAM\[123\]–\[125\]使用测距仪（例如，激光扫描仪、雷达和/或声纳）创建点云表示，但成本高昂且提供的环境信息有限。视觉SLAM（vSLAM）\[118\]，\[119\]使用机载摄像头捕获帧并构建环境的表示。由于其硬件成本低、在小规模场景中精度高以及能够捕获丰富的环境信息，vSLAM已经变得流行。传统的vSLAM技术可以分为传统vSLAM和语义vSLAM\[119\]。传统vSLAM系统使用图像信息和多视图几何原理估计未知环境中机器人的姿态，构建由点云组成的低级地图（例如，稀疏地图、半密集地图和密集地图），如基于滤波器的方法（例如，MonoSLAM\[75\]，MSCKF\[76\]），基于关键帧的方法（例如，PTAM\[77\]，ORB-SLAM\[78\]），和直接跟踪方法（例如，DTAM\[79\]，LSD-SLAM\[80\]）。由于低级地图中的点云不直接对应环境中的对象，这使得它们难以被具身机器人解释和利用。然而，语义概念的出现，特别是与语义信息解决方案集成的语义vSLAM系统，显著提高了机器人感知和导航未探索环境的能力。早期的工作，如SLAM++\[81\]，使用实时3D对象识别和跟踪创建高效的对象图，实现了鲁棒的闭环、重定位和对象检测。CubeSLAM\[82\]和HDP-SLAM\[83\]在地图中引入了3D矩形，构建了一个轻量级的语义地图。QuadricSLAM\[84\]采用语义3D椭球来实现复杂几何环境中对象形状和姿态的精确建模。So-SLAM\[85\]在室内环境中结合了完全耦合的空间结构约束（共面、共线和接近）。为了应对动态环境的挑战，DS-SLAM\[86\]，DynaSLAM\[87\]和SG-SLAM\[88\]采用语义分割进行运动一致性检查和多视图几何算法来识别和过滤动态对象，确保稳定的定位和地图构建。OVD-SLAM\[89\]利用语义、深度和光流信息来区分动态区域，无需预定义标签，实现更准确和鲁棒的定位。GSSLAM\[90\]利用3D高斯表示，通过实时可微的splatting渲染流水线和自适应扩展策略，在效率和准确性之间取得平衡。
    
2.  **3D场景理解**：3D场景理解旨在区分对象的语义、确定它们的位置，并从3D场景数据中推断几何属性，这在自动驾驶\[126\]、机器人导航\[127\]和人机交互\[128\]等方面是基础性的。场景可能使用3D扫描工具（如激光雷达或RGB-D传感器）记录为3D点云。与图像不同，点云是稀疏的、无序的和不规则的\[120\]，这使得场景解释极具挑战性。近年来，提出了许多深度学习方法用于3D场景理解，可以分为基于投影的方法、基于体素的方法和基于点的方法。具体来说，基于投影的方法（例如，MV3D\[91\]，PointPillars\[92\]，MVCNN\[93\]）将3D点投影到各种图像平面上，并使用2D CNN基础结构进行特征提取。基于体素的方法将点云转换为规则的体素网格，以便于3D卷积操作（例如，VoxNet\[94\]，SSCNet\[95\]），一些工作通过稀疏卷积提高效率（例如，MinkowskiNet\[96\]，SSCNs\[97\]，Embodiedscan\[98\]）。相比之下，基于点的方法直接处理点云（例如，PointNet\[99\]，PointNet++\[100\]，PointMLP\[101\]）。最近，为了实现模型的可扩展性，基于Transformers的（例如，PointTransformer\[102\]，Swin3d\[103\]，PT2\[104\]，PT3\[105\]，3D-VisTA\[106\]，LEO\[107\]，PQ3D\[108\]）和基于Mamba的（例如，PointMamba\[109\]，PCM\[110\]，Mamba3D\[111\]）架构已经出现。值得注意的是，除了直接使用点云的特征外，PQ3D\[108\]还无缝结合了多视图图像和体素的特征，以增强场景理解能力。
    
3.  **主动探索**：前面介绍的3D场景理解方法赋予了机器人以被动方式感知环境的能力。在这种情况下，感知系统的信息获取和决策不会随着场景的发展而适应。然而，被动感知为主动探索提供了重要的基础。鉴于机器人能够移动并与周围环境频繁互动，它们也应该能够主动探索和感知环境。图7显示了它们之间的关系。当前的方法通过与环境互动或改变观察方向来获取更多视觉信息 \[112\]，\[113\]。例如，Pinto等人\[112\]提出了一个好奇的机器人，它通过与环境的物理互动学习视觉表示，而不是仅仅依赖于数据集中的类别标签。为了解决不同形态机器人在交互对象感知中的挑战，Tatiya等人\[113\]提出了一个多阶段投影框架，通过学习探索性互动转移隐含知识，使机器人能够在不需要从头开始学习的情况下有效识别对象属性。认识到自动捕获信息性观察的挑战，Jayaraman等人\[114\]提出了一种强化学习方法，其中智能体学习通过减少对环境未观察部分的不确定性来积极获取信息性视觉观察，使用循环神经网络来完成全景场景和3D对象形状的主动完成。NeU-NBV\[115\]引入了一个无地图规划框架，使用基于图像的神经渲染中的新颖不确定性估计来指导数据收集朝着最不确定的视图。Hu等人\[116\]开发了一种机器人探索算法，使用状态价值函数预测未来状态，结合离线蒙特卡洛训练、在线时间差分适应和基于传感器信息覆盖的内在奖励函数。为了解决开放世界环境中意外输入的问题，Fan等人\[117\]将主动识别视为一个顺序证据收集过程，提供逐步不确定性量化和在证据组合理论下的可靠预测，同时通过特别开发的奖励函数在开放世界环境中有效地表征行动的优点。
    

B. 3D视觉定位

与传统的2D视觉定位不同，它在平面图像的限制下操作，3D视觉定位结合了深度、透视和对象之间的空间关系，为智能体与其环境的交互提供了更加强大的框架。3D视觉定位的任务是使用自然语言描述在3D环境中定位对象。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

图8 展示了两阶段（上图）和一阶段（下图）3D视觉定位方法的示意图\[141\]。(a) 展示了3D视觉定位的示例。(b) 两阶段方法包括可能会在检测阶段忽视目标的稀疏提议和可能会在匹配阶段引起混淆的密集提议。(c) 一阶段方法可以根据语言描述的指导逐步选择关键点（蓝点 → 红点 → 绿点）。\[129\]，\[130\]。如表V所总结，3D视觉定位的最新方法大致可以分为两类：两阶段和一阶段方法\[145\]。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

1.  **两阶段3D视觉定位方法**：类似于对应的2D任务\[146\]，早期的3D定位研究主要采用了两阶段检测然后匹配的流程。他们最初使用预训练的检测器\[147\]或分割器\[148\]-\[150\]从3D场景中的许多对象提议中提取特征，然后将这些特征与语言查询特征融合以匹配目标对象。两阶段研究的重点是第二阶段，例如探索对象提议特征与语言查询特征之间的相关性以选择最佳匹配对象。ReferIt3D\[130\]和TGNN\[131\]不仅学习将提议特征与文本嵌入匹配，还通过图神经网络编码对象之间的上下文关系。为了增强自由形式描述和不规则点云中的3D视觉定位，FFL-3DOG\[133\]使用了语言场景图进行短语相关性、多级3D提议关系图以丰富视觉特征，以及描述引导的3D视觉图以编码全局上下文。最近，由于变换器架构在自然语言处理\[151\]、\[152\]和计算机视觉任务\[14\]、\[153\]中展示了出色的性能，研究越来越多地关注使用变换器提取和融合3D视觉定位任务中的视觉语言特征。例如，LanguageRefer\[135\]采用了基于变换器的架构，结合3D空间嵌入、语言描述和类标签嵌入来实现鲁棒的3D视觉定位。3DVG-Transformer\[134\]是一种面向关系的3D点云的视觉定位方法，具有坐标引导的上下文聚合模块，用于关系增强的提议生成和多重注意力模块用于跨模态提议消歧。为了实现对3D对象和指代表达的更细粒度推理，TransRefer3D\[154\]使用实体和关系感知注意力增强跨模态特征表示，结合自注意力、实体感知注意力和关系感知注意力。GPS\[140\]提出了一个统一的学习框架，利用三个层次的对比对齐学习和掩蔽语言建模目标学习，从百万规模的3D视觉-语言数据集（即SCENEVERSE\[140\]）中提取知识。大多数上述方法专注于特定视点，但学到的视觉-语言对应关系在视点变化时可能会失败。为了学习更具视点鲁棒性的视觉表示，MVT\[137\]提出了一种多视图变换器，学习独立于视图的多模态表示。为了减轻稀疏、嘈杂和不完整点云的限制，各种方法探索了结合捕获的（例如，SAT\[132\]或合成的（例如，LAR\[136\]）图像的详细2D视觉特征以增强3D视觉定位任务。现有的3D VG方法通常依赖于大量标记数据进行训练，或在处理复杂语言查询时显示局限性。受到LLMs令人印象深刻的语言理解能力的启发，LLM-Grounder\[138\]提出了一个开放词汇3D视觉定位流程，不需要标记数据，利用LLM分解查询并生成对象识别的计划，然后通过评估空间和常识关系来选择最佳匹配对象。为了捕获视点依赖的查询并解码3D空间中的空间关系，ZSVG3D\[139\]设计了一种零样本开放词汇3D视觉定位方法，使用LLM识别相关对象并执行推理，将此过程转换为脚本化的视觉程序，然后转换为可执行的Python代码以预测对象位置。然而，如图8 (b)所示，这些两阶段方法面临着确定提议数量的困境，因为第一阶段中的3D检测器需要采样关键点来表示整个3D场景，并为每个关键点生成相应的提议。稀疏提议可能会在第一阶段忽视目标，使它们在第二阶段无法匹配。相反，密集提议可能包含不可避免的冗余对象，导致由于过于复杂的提议间关系而在第二阶段难以区分目标。此外，关键点采样策略是与语言无关的，这增加了检测器识别与语言相关的提议的难度。
    
2.  **一阶段3D视觉定位方法**：如图8 (c)所示，与两阶段3D VG方法不同，一阶段3D VG方法整合了由语言查询指导的对象检测和特征提取，使定位对象变得更加容易。3D-SPS\[141\]将3D VG任务视为关键点选择问题，避免了检测和匹配的分离。具体来说，3D-SPS最初通过描述感知关键点采样模块粗略采样与语言相关的关键点。随后，它精细选择目标关键点，并使用目标导向的逐步挖掘模块预测基础。受到MDETR\[155\]和GLIP\[156\]等2D图像语言预训练模型的启发，BUTD-DETR\[142\]提出了一种自下而上的自上而下的检测变换器，可以用于2D和3D VG。具体来说，BUTD-DETR使用标记的自下而上的框提议和自上而下的语言描述来指导通过预测头解码目标对象和相应的语言跨度。
    

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

C. 视觉语言导航（Visual Language Navigation, VLN）

视觉语言导航是具身人工智能（Embodied AI）的关键研究问题，旨在使智能体能够在未见环境中遵循语言指令进行导航。VLN要求机器人不仅要理解复杂多样的视觉观察，而且还要解释不同粒度的指令。VLN的输入通常由两部分组成：视觉信息和自然语言指令。视觉信息可以是过去轨迹的视频，或者是一组历史当前观察图像。自然语言指令包括具身智能体需要到达的目标，或者是预期完成的任务。具身智能体必须使用上述信息从候选列表中选择一个或一系列动作来满足自然语言指令的要求。这个过程可以表示为：

Action\=𝑀(𝑂,𝐻,𝐼)

其中Action是选择的动作或动作候选列表，O是当前观察，H是历史信息，I是自然语言指令。在VLN中最常用的指标有SR（成功率）、TL（轨迹长度）和SPL（由路径长度加权的成功率）。其中，SR直接反映了具身智能体的导航性能，TL反映了导航效率，SPL结合了两者以指示具身智能体的整体性能。下面，我们将VLN分为两部分进行介绍：数据集和方法。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

1.  **数据集**：在VLN中，自然语言指令可以是一系列详细的动作描述、完整描述的目标，或者只是粗略描述的任务，甚至是人类的要求。具身智能体需要完成的任务可能只是单一导航，或者是需要交互的导航，或者是需要依次完成的多个导航任务。这些差异给VLN带来了不同的挑战，并且已经构建了许多不同的数据集。基于这些差异，我们介绍一些重要的VLN数据集。Room to Room (R2R) \[63\] 是基于Matterport3D的VLN数据集。在R2R中，具身智能体根据逐步指令进行导航，根据视觉观察选择下一个相邻的导航图节点，直到到达目标位置。具身智能体需要动态跟踪进度，以使导航过程与细粒度指令对齐。Room-for-Room \[157\] 扩展了R2R中的路径到更长的轨迹，这要求具身智能体具备更强的长距离指令和历史对齐能力。VLN-CE \[158\] 扩展了R2R和R4R到连续环境中，具身智能体可以在场景中自由移动。这使得具身智能体的动作决策更加困难。与基于室内场景的数据集不同，TOUCHDOWN数据集 \[159\] 是基于Google Street View创建的。在TOUCHDOWN中，具身智能体遵循指令，在纽约市的街景渲染模拟中导航以找到指定的对象。与R2R类似，REVERIE数据集 \[160\] 也是基于Matterport3D模拟器构建的。REVERIE要求具身智能体根据简洁的、人类注释的高级自然语言指令准确地定位远处不可见的目标对象，这意味着具身智能体需要在场景中的大量对象中找到目标对象。在SOON \[161\]中，代理接收从粗略到精细的长而复杂的指令，以在3D环境中找到目标对象。在导航过程中，代理首先搜索较大的区域，然后根据视觉场景和指令逐渐缩小搜索范围。这使得DDN \[162\]在这些数据集中更进一步，只提供人类需求而不指定明确的对象。代理需要通过场景导航以找到满足人类需求的对象。ALFRED数据集 \[163\] 是基于AI2-THOR模拟器构建的。在ALFRED中，具身智能体需要理解环境观察，并根据粗粒度和细粒度指令在交互环境中完成家庭任务。OVMM \[164\] 的任务是在任何未见环境中挑选任何对象并将其放置在指定位置。代理需要在家庭环境中定位目标对象，导航并抓取它，然后导航到目标位置放下对象。OVMM提供了基于Habitat的模拟器和在真实世界中实现的框架。Behavior-1K数据集 \[165\] 基于人类需求，包含1000个长序列、复杂的、技能依赖的日常任务，这些任务是在OmniGibson中设计的，OmniGibson是iGibson模拟环境的扩展。代理需要完成包含数千个低级动作步骤的长跨度导航交互任务，这些任务基于视觉信息和语言指令。这些复杂任务需要强大的理解和记忆能力。还有一些更特殊的数据集。CVDN \[166\] 要求具身智能体根据对话历史导航到目标，并在不确定时提出问题以获取帮助以决定下一个动作。DialFRED \[167\] 是ALFRED的扩展，允许代理在导航和交互过程中提问以获得帮助。这些数据集都引入了额外的预言者，具身智能体需要通过提问获取对导航有益的更多信息。
    
2.  **方法**：随着LLMs的惊人表现，VLN的方向和焦点已经发生了深刻的变化。尽管如此，VLN方法可以分为两个方向：基于记忆理解和基于未来预测。基于记忆理解的方法侧重于环境的感知和理解，以及基于历史观察或轨迹的模型设计，这是一种基于过去学习的方法。基于未来预测的方法更加关注建模、预测和理解未来状态，这是一种未来学习的方法。由于VLN可以被视为部分可观测的马尔可夫决策过程，其中未来的观察依赖于当前的环境和智能体的动作，历史信息对导航决策具有重要意义，尤其是长跨度导航决策，因此基于记忆理解的方法一直是VLN的主流。然而，基于未来预测的方法仍然具有重要意义。其对环境的基本理解在连续环境中的VLN具有巨大价值，特别是随着世界模型概念的兴起，基于未来预测的方法正受到越来越多的研究关注。
    

**基于记忆理解的。**基于图的学习是记忆理解方法的重要组成部分。基于图的学习通常以图的形式表示导航过程，其中具身智能体在每个时间步骤获得的信息被编码为图的节点。具身智能体获得全局或部分导航图信息作为历史轨迹的表示。LVERG \[168\] 分别对每个节点的语言信息和视觉信息进行编码，设计了一个新的语言和视觉实体关系图来模拟文本和视觉之间的跨模态关系以及视觉实体之间的内模态关系。LM-Nav \[172\] 使用目标条件距离函数推断原始观察集之间的连接，并构建导航图，并从指令中提取地标，通过视觉语言模型将其与导航图的节点匹配。尽管HOP \[173\] 不是基于图学习，但其方法类似于图，要求模型对不同粒度的时间有序信息进行建模，从而实现对历史轨迹和记忆的深入理解。导航图将环境离散化，但同时理解和编码环境也很重要。FILM \[171\] 在导航过程中使用RGB-D观察和语义分割从3D体素逐步构建语义地图。VER \[178\] 通过2D-3D采样将物理世界量化为结构化的3D单元，提供细粒度的几何细节和语义。不同的学习方案探索了如何更好地利用历史轨迹和记忆。通过对抗学习，CMG \[169\] 在模仿学习和探索鼓励方案之间交替，有效地加强了对指令和历史轨迹的理解，缩短了训练和推理之间的差异。GOAT \[177\] 直接通过后门调整因果学习（BACL）和前门调整因果学习（FACL）训练无偏模型，并通过对视觉、导航历史及其与指令的组合进行对比学习，使智能体能够更充分地利用信息。RCM \[170\] 提出的增强跨模态匹配方法使用目标导向的外部奖励和指令导向的内部奖励进行全局和局部的跨模态定位，并通过自监督模仿学习从其自身的历史良好决策中学习。FSTT \[175\] 引入了TTA到VLN，并在两个时间步骤和任务的尺度上优化了模型的梯度和模型参数，有效提高了模型性能。在记忆理解方法中，大型模型的具体应用是理解历史记忆的表示，并基于其广泛的世界知识理解环境和任务。NaviLLM \[174\] 通过视觉编码器将历史观察序列集成到嵌入空间，将融合编码的多模态信息输入到大型模型并进行微调，在多个基准测试中达到了最先进的水平。NaVid \[179\] 在历史信息编码方面进行了改进，通过不同程度的池化在历史观察和当前观察之间实现了不同程度的信息保留。DiscussNav \[176\] 为大型模型专家分配了不同的角色，驱动大型模型在导航动作前进行讨论以完成导航决策，并在零样本VLN中取得了优异的性能。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

**基于未来预测的。**基于图的学习也广泛应用于基于未来预测的方法中。BGGL \[182\] 和 ETPNav \[185\] 使用了类似的设计方法，设计了一个航点预测器，可以根据当前导航图节点的观察在连续环境中预测可移动路径点。他们的目标是将复杂环境中的导航从连续环境转移到离散环境中的节点到节点导航，从而弥合从离散环境到连续环境的性能差距。通过环境编码提高对未来环境的理解和感知也是预测和探索未来状态的研究方向之一。NvEM \[181\] 使用主题模块和参考模块从全局和局部视角对邻居视图进行融合编码。这实际上是对未来观察的理解和学习。HNR \[184\] 使用大规模预训练的分层神经辐射表示模型直接预测未来环境的视觉表示，而不是像素级图像，使用三维特征空间编码，并基于未来环境的表示构建可导航的路径树。他们从不同的层次预测未来环境，为导航决策提供有效的参考。一些强化学习方法也被应用于预测和探索未来状态。LookBY \[180\] 采用强化预测，使预测模块能够模仿世界并预测未来状态和奖励。这允许智能体直接将“当前观察”和“未来观察的预测”映射到动作上，在当时达到了最先进的性能。大型模型的丰富世界知识和零样本性能为基于未来预测的方法提供了许多可能性。MiC \[183\] 要求大型模型直接从指令中预测目标及其可能的位置，并通过场景感知的描述提供导航指令。这种方法要求大型模型充分发挥其“想象力”，并通过提示构建一个想象中的场景。此外，还有一些方法既从过去学习又为未来学习。MCR-Agent \[186\] 设计了一个三层动作策略，要求模型从指令中预测目标，预测目标的像素级掩模以进行交互，并从以前的导航决策中学习；OVLM \[187\] 要求大型模型为指令预测相应的操作和地标序列。在导航过程中，视觉语言地图将不断更新和维护，并将操作链接到地图上的航点。

D. 非视觉感知：触觉

触觉传感器为智能体提供了如纹理、硬度和温度等详细信息。对于相同的动作，从视觉和触觉传感器中学到的知识可能是相关和互补的，使机器人能够充分掌握手中的高精度任务。因此，触觉感知对于物理世界中的智能体至关重要，并且无疑增强了人机交互\[188\]–\[190\]。对于触觉感知任务，智能体需要从物理世界中收集触觉信息，然后执行复杂任务。在这一部分中，如图10所示，我们首先介绍现有的触觉传感器类型及其数据集，然后讨论触觉感知中的三个主要任务：估计、识别和操纵。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

1) **传感器设计**：人类触觉的原理是皮肤在触摸时发生形状变化，其丰富的神经细胞发送电信号，这也为设计触觉传感器提供了基础。触觉传感器设计方法可以分为三类：非视觉基础、视觉基础和多模态。非视觉基础触觉传感器主要使用电力和机械原理，主要注册基本的、低维的感官输出，如力量、压力、振动和温度\[191\]–\[196\]。其中一个著名的代表是BioTac及其模拟器\[197\]\[198\]。基于视觉的触觉传感器基于光学原理。使用凝胶变形的图像作为触觉信息，基于视觉的触觉传感器如GelSight、Gelslim、DIGIT、9DTact、TacTip、GelTip和AllSight已被用于众多应用。模拟器如TACTO和Taxim也很受欢迎。最近的工作集中在降低成本\[202\]和集成到机器人手中\[201\]\[208\]\[209\]。多模态触觉传感器，受人类皮肤的启发，结合了压力、接近度、加速度和温度等多种模态信息，使用柔性材料和模块化设计。

2) **数据集**：非视觉传感器的数据集主要由BioTac系列收集\[197\]，包含电极值、力量向量和接触位置。由于任务主要是力量和抓取细节的估计，数据集中的对象通常是力量和抓取样本。基于视觉的传感器，拥有高分辨率的变形凝胶图像，更侧重于更高的估计、纹理识别和操纵。数据集由Geisight传感器、DIGIT传感器及其模拟器收集\[199\]\[201\]\[202\]\[206\]，包括家庭对象、野外环境、不同材料和抓取项目。由于图像信息可以很容易地与其他模态（图像、语言、音频等）对齐和绑定\[14\]\[210\]，具身代理中的触觉感知主要围绕基于视觉的传感器。我们介绍了十个主要的触觉数据集，总结在表VIII中。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

3) **方法**：触觉感知有众多应用，可以分为三类：估计、精确的机器人操纵和多模态识别任务。

**a) 估计**：早期的估计工作主要集中在基本算法的形状、力量和滑动测量\[202\]\[220\]\[221\]。研究人员简单地使用阈值或应用卷积神经网络来解决这些任务，基于触觉图像的颜色和标记在不同帧中的分布变化。估计工作的重点是第二阶段，触觉图像的生成和对象的重建。触觉图像的生成\[222\]–\[225\]旨在从视觉数据生成触觉图像。起初应用了深度学习模型，输入RGB-D图像并输出触觉图像\[222\]\[223\]。最近，随着图像生成的快速发展，Higuera等人\[224\]和Yang等人\[225\]应用了扩散模型进行触觉生成，并且表现良好。对象的重建可以分为2D重建\[226\]\[227\]和3D重建\[202\]\[219\]\[228\]–\[241\]。2D重建主要关注对象的形状和分割，而3D重建关注表面和姿态，甚至是完整的场景感知。任务首先采用了数学方法、自编码器方法和神经网络方法来融合视觉（有时是点云）和触觉特征。最近，研究人员如Comi等人\[236\]和Dou等人\[219\]在触觉重建工作中实现了基于神经辐射场（NeRF）和3D高斯溅射（3DGS）的新方法。

**b) 机器人操纵**：在触觉任务中，弥合仿真与现实之间的差距至关重要。提出了强化学习和基于GAN的方法来解决准确、及时的机器人操纵任务中的变异问题。

强化学习方法。Visuotactile-RL\[242\]为现有RL方法提出了几种方法，包括触觉门控、触觉数据增强和视觉退化。Rotateit\[243\]是一个系统，它利用多模态感官输入实现基于指尖的对象旋转，沿多个轴进行。它通过网络训练强化学习策略，使用特权信息，并启用在线推理。\[244\]提出了一种使用触觉感知进行目标推动的深度RL方法。它提出了一个目标条件公式，允许无模型和基于模型的RL获得推动对象到目标的准确策略。AnyRotate\[245\]专注于手内操纵。它是一个使用密集特征的触觉反馈进行重力不变多轴手内对象旋转的系统，在模拟中构建了连续的接触特征表示，以提供触觉反馈，用于训练策略，并引入了一种通过训练观察模型来执行零样本策略迁移的方法。

基于GAN的方法。ACTNet\[246\]提出了一种无监督对抗性领域适应方法，以缩小像素级触觉感知任务的领域差距。引入了一种自适应相关注意力机制来改进生成器，使其能够利用全局信息并专注于显著区域。然而，像素级领域适应导致误差累积、性能下降、结构复杂性和训练成本增加。相比之下，STR-Net\[247\]提出了一种针对触觉图像的特征级无监督框架，缩小了特征级触觉感知任务的领域差距。此外，一些方法专注于仿真到现实。例如，Tactile Gym 2.0\[248\]。然而，由于其复杂性和高成本，它在实际应用中具有挑战性。

c) 识别： 触觉表示学习侧重于材料分类和多模态理解，可以分为两类：传统方法和大型语言模型（LLMs）及视觉-语言模型（VLMs）方法。

传统方法： 为了增强触觉表示学习，采用了多种传统方法。自动编码器框架在开发紧凑的触觉数据表示方面发挥了重要作用。Polic等人\[249\]使用卷积神经网络自动编码器对基于光学的触觉传感器图像进行降维。Gao等人\[250\]创建了一个受监督的循环自动编码器来处理异构传感器数据集，而Cao等人\[251\]创建的TacMAE使用了一个掩蔽自动编码器来处理不完整的触觉数据。Zhang等人\[252\]引入了MAE4GM，这是一个整合视觉和触觉数据的多模态自动编码器。由于触觉作为其他模态的补充，联合训练方法被用来融合多个模态。Yuan等人\[253\]使用包括深度、视觉和触觉数据的模态训练CNN。同样，Lee等人\[254\]使用了变分贝叶斯方法来处理力传感器系列和末端执行器指标等模态。为了更好地学习表示，自监督方法如对比学习也是将模态结合在一起的关键技术。在对比方法的研究中，Lin等人\[255\]将触觉输入简单地与多个视觉输入配对，而Yang等人\[256\]采用了视觉触觉对比多视图特征。Kerr等人\[215\]使用了InfoNCE损失，Guzey等人\[257\]使用了BYOL。这些传统方法为触觉表示学习奠定了坚实的基础。

LLMs和VLMs方法： LLM和VLM最近表现出对跨模态交互的惊人理解，并展示了强大的零样本性能。最近的工作，如Yang等人\[189\]、Fu等人\[218\]和Yu等人\[258\]，通过对比预训练方法对触觉数据进行编码和与视觉和语言模态对齐。然后，像LLaMA这样的大型模型将被应用，使用微调方法来适应触觉描述等任务。LLM和VLM技术的出现进一步推进了该领域，使得跨模态触觉表示更加全面和健壮。

4) 困难：a) 不同传感器类型的缺陷：传统传感器提供简单和低维数据，对多模态学习构成挑战。基于视觉的传感器和电子皮肤虽然高度准确，但成本昂贵。b) 数据获取挑战：收集数据，特别是同时收集触觉和视觉数据，尽管在开发简化收集设备方面取得了一些进展，但仍然困难。c) 标准不一致：触觉传感器以不一致的标准和原则运作，阻碍了大规模学习和限制了公共数据集的有用性。需要标准化和广泛的数据集。

V. 具身交互

具身交互任务指的是智能体在物理或模拟空间中与人类和环境进行交互的场景。典型的具身交互任务包括具身问答（EQA）和具身抓取。

A. 具身问答 (Embodied Question Answering)

对于EQA任务，智能体需要从第一人称视角探索环境，以收集回答给定问题所需的信息。具有自主探索和决策能力的智能体不仅要考虑采取哪些行动来探索环境，还要决定何时停止探索以回答问题。现有的工作集中在不同类型的问题上，如图11所示。在这一部分中，我们将介绍现有的数据集，讨论相关的方法，描述用于评估模型性能的指标，并解决这项任务的剩余限制。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

**1) 数据集**：在真实环境中进行机器人实验通常受到场景和机器人硬件的限制。作为虚拟实验平台，模拟器为构建具身问答数据集提供了合适的环境条件。在模拟器中创建的数据集上训练和测试模型显著降低了实验成本，并提高了在真实机器上部署模型的成功率。我们简要介绍了几个具身问答数据集，总结在表IX中。EQA v1 \[259\] 是为EQA设计的首个数据集。它建立在House3D \[269\] 模拟器中基于SUNCG数据集 \[95\] 的合成3D室内场景上，包含四种类型的问题：位置、颜色、颜色房间和介词。它拥有超过5000个问题，分布在750多个环境中。这些问题是通过功能程序执行构建的，使用模板选择和组合基本操作。与EQA v1类似，MT-EQA \[260\] 也是在House3D中使用SUNCG构建的，但它将单对象问答任务扩展到了多对象设置。设计了六种类型的问题，涉及多个对象之间的颜色、距离和大小比较。数据集包含588个环境中的19,287个问题。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

**2) 方法**：具身问题回答任务主要涉及导航和问题回答子任务，实现方法大致分为两类：基于神经网络的方法和基于大型语言模型(LLMs)/视觉-语言模型(VLMs)的方法。

神经网络方法： 在早期工作中，研究人员主要通过构建深度神经网络来解决具身问题回答任务。他们使用模仿学习、强化学习等技术来训练和微调这些模型，以提高性能。Das等人\[259\]首次提出了EQA任务，其中智能体由四个主要模块组成：视觉、语言、导航和回答。这些模块主要使用传统的神经网络构建块：卷积神经网络(CNN)和循环神经网络(RNN)。它们分两个阶段进行训练。最初，导航和回答模块独立地在自动生成的专家导航演示上使用模仿或监督学习进行训练。随后，在第二阶段，使用策略梯度对导航架构进行微调。一些后续工作\[274\]，\[275\]保留了Das等人\[259\]提出的问题回答模块，并改进了模型。此外，Wu等人\[275\]提出将导航和QA模块整合到统一的SGD训练流程中进行联合训练，从而避免了同时使用深度强化学习来训练分开训练的导航和问题回答模块。还有一些工作尝试增加问题回答任务的复杂性和完整性。从任务单一性的角度来看，一些工作\[260\]，\[276\]将任务扩展到包括多个目标和多智能体，分别要求模型通过特征提取和场景重建等方法存储和整合智能体探索过程中获得的信息。考虑到智能体与动态环境的交互，Gordon等人\[262\]引入了分层交互记忆网络。控制权在规划器和执行任务的低级控制器之间交替，在此过程中使用了一个以自我为中心的空间GRU(esGRU)来存储空间记忆，使智能体能够导航并提供答案。之前工作中的一个限制是智能体无法使用外部知识回答复杂问题，并且缺乏对探索过的场景部分的知识。为了解决这个问题，Tan等人\[265\]提出了一个框架，利用神经程序合成方法和从知识和3D场景图中转换的表格，允许动作规划器访问与对象相关的信息。此外，还使用了基于蒙特卡洛树搜索(MCTS)的方法来确定智能体下一步移动的位置。  

LLMs/VLMs方法： 近年来，LLMs和VLMs不断取得进展，并在各个领域展示了卓越的能力。因此，研究人员尝试将这些模型应用于解决具身问题回答任务，而无需任何额外的微调。

Majumdar等人\[266\]探索了使用LLMs和VLMs进行情景记忆EQA(EM-EQA)任务和活跃EQA(AEQA)任务。对于EM-EQA任务，他们考虑了盲目LLMs、具有情景记忆语言描述的苏格拉底LLMs、具有构建场景图描述的苏格拉底LLMs，以及处理多个场景帧的VLMs。AEQA任务扩展了EM-EQA方法，增加了基于前沿的探索(FBE)\[277\]，用于问题无关的环境探索。一些其他工作\[267\]，\[278\]也采用了基于前沿的探索方法来识别后续探索的区域，并构建语义地图。他们使用一致性预测或图像-文本匹配提前结束探索，以避免过度探索。Patel等人\[279\]强调了任务的问题回答方面。他们利用多个基于LLM的智能体探索环境，并使它们能够独立回答“是”或“否”的问题。这些个别回答被用来训练一个中央答案模型，负责聚合回答并生成健壮的答案。

3) 指标： 性能通常基于两个方面进行评估：导航和问题回答。在导航中，许多工作遵循了Das等人\[259\]介绍的方法，并使用指标如完成导航时到达目标对象的距离(dT)、从初始位置到最终位置目标距离的变化(d∆)以及在情节中的任何时候到达目标的最小距离(dmin)来评估模型的性能。它们在距离目标10、30或50个动作时进行测试。还有一些工作基于指标如轨迹长度、目标对象的交并比分数(IoU)等进行测量。对于问题回答，评估主要涉及答案列表中真实答案的平均排名(MR)和答案的准确性。最近，Majumdar等人\[266\]引入了基于LLM的聚合正确性指标(LLM-Match)的概念，以评估开放词汇答案的准确性。此外，他们通过将智能体路径的规范化长度作为权重纳入正确性指标，来评估效率。

VI. 具身智能体智能体被定义为能够感知其环境并采取行动以实现特定目标的自主实体。最近在多模态大型模型（MLMs）方面的进展进一步扩展了智能体在实际场景中的应用。当这些基于MLM的智能体被具身化到物理实体中时，它们能够有效地将它们的能力从虚拟空间转移到物理世界，从而成为具身智能体\[298\]。图13显示了具身智能体的时间线概述。为了使具身智能体能够在信息丰富且复杂的现实世界中操作，具身智能体已经被开发出强大的多模态感知、交互和规划能力，如图14所示。为了完成一个任务，具身智能体通常涉及以下过程：1）将抽象和复杂的任务分解为特定的子任务，这被称为高水平的具身任务规划。2）通过有效利用具身感知和具身交互模型或利用基础模型的策略功能逐步实施这些子任务，称为低水平的具身动作规划。值得注意的是，任务规划涉及行动前的思考，因此通常被认为是在网络空间中进行的。相比之下，动作规划必须考虑与环境的有效交互以及将这些信息反馈给任务规划者以调整任务规划。因此，对具身智能体来说，将其能力从网络空间对齐和概括到物理世界至关重要。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

A. 具身多模态基础模型

具身智能体需要在视觉上识别其环境，通过听觉理解指令，并理解自身的状态，以实现复杂的交互和操作。这要求一个模型整合多种感官模态和自然语言处理能力，通过综合不同类型的数据来增强智能体的理解和决策。因此，出现了具身多模态基础模型。最近，Google DeepMind发现利用基础模型和大型、多样化的数据集是最佳策略。他们基于机器人变换器（RT）\[11\]开发了一系列工作，为未来具身智能体研究提供了重要见解。在基础机器人模型方面取得了显著进展，从最初的SayCan \[299\]开始，该模型使用三个独立的模型进行规划、可承受性和低级策略。Q-Transformer \[300\]后来统一了可承受性和低级策略，PaLM-E \[301\]集成了规划和可承受性。然后，RT-2 \[302\]通过将所有三个功能整合到一个单一模型中，实现了突破，使联合扩展和正向迁移成为可能。这代表了机器人基础模型的显著进步。RT-2引入了视觉-语言-动作（VLA）模型，具有“思维链”推理能力，能够进行多步骤的语义推理，如在各种情境中选择替代工具或饮料。最终，RT-H \[4\]实现了具有动作层次结构的端到端机器人变换器，以细粒度推理任务规划。

为了解决具身模型的泛化限制，Google与33个领先的学术机构合作，创建了全面的开放X-Embodiment数据集\[303\]，整合了22种不同的数据类型。使用这个数据集，他们训练了通用大型模型RT-X。这也促进了更多的开源VLMs参与机器人领域，如基于LLaVA的EmbodiedGPT \[304\]和基于FLAMINGO的RoboFlamingo \[305\]。尽管开放X-Embodiment提供了大量数据集，但考虑到具身机器人平台的快速发展，构建数据集仍然是一个挑战。为了解决这个问题，AutoRT \[306\]创建了一个系统，用于在新环境中部署机器人以收集训练数据，利用LLMs通过更全面和多样化的数据来增强学习能力。

另外，基于变换器的架构面临效率问题，因为具身模型需要长上下文，包括来自视觉、语言和具身状态的信息，以及与当前执行任务相关的内存。例如，尽管RT-2表现出色，但其推理频率仅为1-3Hz。已经做出了一些努力，如通过量化和蒸馏部署模型。此外，改进模型框架是另一种可行的方法。SARA-RT \[307\]采用了更高效的线性注意力，而RoboMamba \[308\]利用了更适合长序列任务的mamba架构，使其推理速度比现有的机器人MLMs快七倍。基于生成模型的RT擅长高层次任务理解和规划，但在低层次动作规划方面存在局限性，因为生成模型无法精确生成动作参数以及高层次任务规划与低层次动作执行之间的差距。为了解决这个问题，Google引入了RT-Trajectory \[309\]，通过自动添加机器人轨迹为学习机器人控制策略提供低层次视觉线索。同样，基于RT-2框架，机器人变换器动作层次结构（RT-H）通过中间语言动作将高层次任务描述与低层次机器人动作联系起来\[4\]。此外，VLA模型仅在与VLMs相关的高层次规划和可承受性任务中表现出能力，它们未能在低层次物理交互中展示新技能，并受到其数据集中技能类别的限制，导致动作笨拙。未来的研究应该将强化学习整合到大型模型的训练框架中，以提高泛化能力，使VLA模型能够在现实世界环境中自主学习和优化低层次物理交互策略，从而更灵活、更准确地执行各种物理动作。

B. 具身任务规划

如前所述，对于任务“把苹果放在盘子上”，任务规划器将其分解为子任务“找到苹果，拿起苹果”和“找到盘子”、“放下苹果”。由于如何找到（导航任务）或拿起/放下动作（抓取任务）不在任务规划的范围之内。这些动作通常在模拟器中预定义或使用预训练策略模型在真实场景中执行，例如使用CLIPort \[294\]进行抓取任务。传统的具身任务规划方法通常基于显式规则和逻辑推理。例如，使用符号规划算法如STRIPS \[310\]和PDDL \[311\]，以及搜索算法如MCTS \[312\]和A\* \[313\]来生成计划。然而，这些方法通常依赖于预定义的规则、约束和启发式，这些规则是固定的，可能无法很好地适应环境的动态或不可预见的变化。随着LLMs的普及，许多工作尝试使用LLMs进行规划或将传统方法与LLMs结合，利用它们内部丰富的世界知识进行推理和规划，无需手工定义，大大增强了模型的泛化能力。

1.  **利用LLMs的紧急能力进行规划**：在自然语言模型扩大规模之前，任务规划器通过训练像BERT这样的模型在具身指令数据集上，如Alfred \[314\]和Alfworld \[315\]，由FILM \[316\]展示。然而，这种方法受到训练集示例的限制，无法有效与物理世界对齐。现在，由于LLMs的紧急能力，它们可以使用内部世界知识进行任务分解，并通过思维链推理，类似于人类在行动前的推理过程。例如，Translated LM \[317\]和Inner Monologue \[318\]可以将复杂任务分解为可管理的步骤，并使用内部逻辑和知识体系制定解决方案，无需额外训练，如ReAct \[319\]。同样，多智能体协作框架ReAd \[320\]提出了通过不同提示进行有效自我完善计划的方法。此外，一些方法将过去成功的示例抽象为一系列技能存储在记忆库中，在推理期间考虑以提高规划成功率\[321\]–\[323\]。一些工作使用代码作为推理媒介而不是自然语言，任务规划根据可用的API库生成代码\[324\]–\[326\]。此外，多轮推理可以有效地纠正任务规划中的潜在幻觉，这是许多基于LLM的智能体研究的重点。例如，Socratic Models \[327\]和Socratic Planner \[328\]使用苏格拉底式提问来得出可靠的计划。然而，在任务规划期间，执行期间可能出现潜在故障，通常由于规划器没有完全考虑真实环境的复杂性和任务执行的困难\[318\]，\[329\]。由于缺乏视觉信息，计划的子任务可能与实际场景偏离，导致任务失败。因此，将视觉信息整合到规划或执行期间的重新规划中是必要的。这种方法可以显著提高任务规划的准确性和可行性，更好地应对真实世界环境的挑战。
    
2.  **利用具身感知模型的视觉信息进行规划**：基于上述讨论，将视觉信息进一步整合到任务规划（或重新规划）中尤为重要。在此过程中，由视觉输入提供的对象标签、位置或描述可以为LLMs的任务分解和执行提供关键参考。通过视觉信息，LLMs可以更准确地识别当前环境中的目标对象和障碍物，从而优化任务步骤或修改子任务目标。一些工作使用对象检测器在任务执行期间查询环境中存在的对象，并将此信息反馈给LLM，允许它修改当前计划中的不合理步骤\[327\]，\[329\]，\[330\]。RoboGPT考虑了同一任务中相似对象的不同名称，进一步提高了重新规划的可行性\[10\]。然而，标签提供的信息仍然过于有限。可以提供更多的场景信息吗？SayPlan \[331\]提出使用分层3D场景图来表示环境，有效缓解了在大型、多层和多房间环境中进行任务规划的挑战。同样，ConceptGraphs \[332\]也采用3D场景图向LLMs提供环境信息。与SayPlan相比，它提供了更详细的开放世界对象检测，并将任务规划以基于代码的格式呈现，这更有效，更适合复杂任务的需求。然而，有限的视觉信息可能导致智能体对其环境的理解不足。虽然LLMs获得了视觉提示，但它们常常无法捕捉到环境的复杂性和动态变化，导致误解和任务失败。例如，如果一条毛巾被锁在浴室柜里，智能体可能会在浴室里反复搜寻而不考虑这种可能性\[10\]。为了解决这个问题，必须开发更强大的算法来整合多种感官数据，增强智能体对环境的理解。此外，利用历史数据和上下文推理，即使在视觉信息有限的情况下，也可以帮助智能体做出合理的判断和决策。这种多模态整合和基于上下文的推理方法不仅提高了任务执行的成功率，而且为具身人工智能的发展提供了新的视角。
    
3.  **利用VLMs进行规划**：与使用外部视觉模型将环境信息转换为文本不同，VLM模型可以在潜在空间中捕捉视觉细节，特别是难以用对象标签表示的上下文信息。VLM能够识别视觉现象背后的规则；例如，即使毛巾在环境中不可见，也可以推断毛巾可能存放在柜子里。这个过程本质上展示了如何在潜在空间中更有效地对齐抽象的视觉特征和结构化的文本特征。在EmbodiedGPT \[304\]中，Embodied-Former模块对齐了具身的、视觉的和文本的信息，在任务规划期间有效考虑了智能体的状态和环境信息。与直接使用第三人称视角图像的EmbodiedGPT不同，LEO \[333\]将2D自我中心图像和3D场景编码为视觉标记。这种方法有效地感知了3D世界信息并据此执行任务。同样，EIF-Unknow模型使用从体素特征中提取的语义特征图作为视觉标记，这些标记与文本标记一起输入到训练有素的LLaVA模型中进行任务规划\[334\]。此外，具身多模态基础模型，或VLA模型，已经在RT系列\[11\]、\[302\]、PaLM-E \[301\]和Matcha \[335\]等研究中通过大型数据集进行了广泛训练，以实现在具身场景中视觉和文本特征的对齐。然而，任务规划只是智能体完成指令任务的第一步；随后的动作规划决定了任务是否可以完成。在RoboGPT \[10\]的实验中，任务规划的准确率达到了96%，但整体任务完成率仅为60%，受到低级规划器性能的限制。因此，具身智能体能否从“想象任务如何完成”的网络空间过渡到“与环境互动并完成任务”的物理世界，取决于有效动作规划。
    

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

C. 具身动作规划

第VI-B节讨论了任务规划和动作规划的定义和区别。显然，动作规划必须解决现实世界的不确定性，因为任务规划提供的子任务粒度不足以指导智能体在环境互动中。通常，智能体可以通过两种方式实现动作规划：1) 使用预训练的具身感知和具身干预模型作为工具，通过API逐步完成任务规划指定的子任务，2) 利用VLA模型的固有能力派生动作规划。此外，动作规划器的执行结果反馈给任务规划器以调整和改进任务规划。

1.  **使用API的动作**：一种典型的方法是为LLMs提供各种训练有素的政策模型的定义和描述作为上下文，使它们能够理解这些工具并决定如何以及何时为特定任务调用它们\[299\]，\[329\]。此外，通过生成代码，一系列更细粒度的工具可以抽象成一个函数库以供调用，而不是直接传递子任务所需的参数给导航和抓取模型\[326\]。考虑到环境的不确定性，Reflexion可以进一步调整这些工具以实现更好的泛化\[336\]。优化这些工具可以增强智能体的鲁棒性，并且可能需要新工具来完成未知任务。DEPS在零样本学习的前提条件下，赋予LLMs各种角色设置，以学习在与环境互动时的各种技能。在随后的互动中，LLMs可以学习选择和组合这些技能以发展新技能\[337\]。这种分层规划范式允许智能体专注于高层次任务规划和决策，同时将特定动作执行委托给策略模型，简化了开发过程。任务规划器和动作规划器的模块化使它们能够独立开发、测试和优化，增强了系统的灵活性和可维护性。这种方法允许智能体通过调用不同的动作规划器适应各种任务和环境，并促进修改，而无需对智能体的结构进行重大更改。然而，调用外部策略模型可能会引入延迟，可能影响响应时间和效率，特别是在实时任务中。智能体的性能严重依赖于策略模型的质量。如果策略模型无效，智能体的整体性能将受到影响。
    
2.  **使用VLA模型的动作**：与之前在同一系统中执行任务规划和动作执行的方法不同，这种范式利用具身多模态基础模型的能力进行规划和执行动作，减少了通信延迟并提高了系统响应速度和效率。在VLA模型中，感知、决策和执行模块的紧密整合使系统能够更有效地处理复杂任务并适应动态环境的变化。这种整合还有助于实时反馈，使智能体能够自我调整策略，从而增强任务执行的鲁棒性和适应性\[3\]，\[303\]，\[304\]。然而，这种范式无疑更加复杂和昂贵，特别是处理复杂或长期任务时。此外，一个关键问题是，没有具身世界模型的动作规划器，仅凭LLM的内部知识无法模拟物理定律。这个限制阻碍了智能体在物理世界中准确有效地完成各种任务，阻止了从网络空间到物理世界的无缝转移。
    

VII. 仿真到现实适应性（Sim-to-Real Adaptation）

在具身人工智能中，仿真到现实适应性指的是将学习能力或行为从模拟环境（网络空间）转移到现实世界场景（物理世界）的过程。它涉及验证和改进在模拟中开发的算法、模型和控制策略的有效性，以确保它们在物理环境中表现出鲁棒性和可靠性。为了实现仿真到现实适应性，具身世界模型、数据收集和训练方法以及具身控制算法是三个基本组成部分。  

A. 具身世界模型（Embodied World Model）

仿真到现实涉及创建与现实世界环境非常相似的模拟世界模型，帮助算法在转移时更好地泛化。世界模型方法旨在构建一个端到端的模型，通过生成或预测的方式，将视觉映射到动作，甚至任何输入到任何输出，以做出决策。这类世界模型与VLA模型的最大区别在于，VLA模型首先是在大规模互联网数据集上训练以获得高水平的紧急能力，然后与现实世界的机器人数据共同微调。相比之下，世界模型是从物理世界数据从头开始训练的，随着数据量的增加逐渐发展出高级能力。然而，它们仍然是低级的物理世界模型，有点像人类神经反射系统的工作机制。这使它们更适合于输入和输出相对结构化的场景，如自动驾驶（输入：视觉，输出：油门、刹车、方向盘）或物体排序（输入：视觉、指令、数值传感器，输出：抓取目标物体并将其放置在目标位置）。它们不太适合于泛化到结构化、复杂的具身任务。在物理模拟领域，学习世界模型是有希望的。与传统的模拟方法相比，它提供了显著的优势，例如能够在不完整信息下推理交互、满足实时计算需求，并随着时间的推移提高预测准确性。这种世界模型的预测能力至关重要，它使机器人能够发展出在人类世界中操作所需的物理直觉。如图15所示，根据世界环境的学习流程，它们可以分为基于生成的方法、基于预测的方法和知识驱动的方法。我们在表XI中简要总结了提到的方法。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

1.  基于生成的方法（Generation-based Methods）：随着模型规模和数据的逐步增加，生成模型已经展示了理解和生成符合物理定律的图像（例如，World Models \[338\]）、视频（例如，Sora \[17\]、Pandora \[339\]）、点云（例如，3D-VLA \[340\]）或其他格式数据（例如，DWM \[341\]）的能力。这表明生成模型能够学习并内化世界知识。具体来说，经过大量数据的暴露后，生成模型不仅能捕捉数据的统计特性，还能通过其内在结构和机制模拟真实世界的物理和因果关系。因此，这些生成模型可以被视为不仅仅是简单的模式识别工具：它们表现出世界模型的特征。因此，生成模型中嵌入的世界知识可以被利用来提高其他模型的性能。通过挖掘和利用生成模型中表示的世界知识，我们可以提高模型的泛化能力和鲁棒性。这种方法不仅增强了模型对新环境的适应性，还提高了对未知数据的预测准确性\[339\]、\[340\]。然而，生成模型也有一些限制和缺点。例如，当数据分布显著偏斜或训练数据不足时，生成模型可能会产生不准确或失真的输出。此外，这些模型的训练过程通常需要大量的计算资源和时间，模型通常缺乏可解释性，这使得它们的实际应用变得复杂。总的来说，虽然生成模型在理解和生成符合物理定律的内容方面展示了巨大的潜力，但要有效应用它们，必须解决几个技术和实际挑战。这些挑战包括提高模型效率、增强可解释性以及解决与数据偏差相关的问题。随着研究和发展的进行，预计生成模型在未来的应用中将展示出更大的价值和潜力。
    
2.  基于预测的方法（Prediction-based Methods）：基于预测的世界模型通过构建和利用内部表示来预测和理解环境。通过根据给定条件在潜在空间重建相应的特征，它捕获了更深层次的语义和相关的世界知识。这个模型将输入信息映射到潜在空间，并在该空间内操作，提取和利用高级语义信息，从而使机器人能够更准确地感知世界环境的基本表示（例如，I-JEPA \[16\]、MC-JEPA \[342\]、A-JEPA \[343\]、Point-JEPA \[354\]、IWM \[344\]）并更准确地执行具身下游任务（例如，iVideoGPT \[345\]、IRASim \[346\]、STP \[347\]、MuDreamer \[348\]）。与传统的像素级信息相比，潜在特征可以抽象并解耦各种形式的知识，使模型能够更有效地处理复杂任务和场景，并提高其泛化能力\[355\]。例如，在时空建模中，世界模型需要根据对象的当前状态和交互的性质预测其交互后的后状态，将这些信息与其内部知识结合起来。
    
3.  知识驱动的方法（Knowledge-driven Methods）： 知识驱动的世界模型将人工构建的知识注入模型中，赋予它们世界知识。这种方法在具身人工智能领域显示出广泛的应用潜力。例如，在real2sim2real方法\[357\]中，使用真实世界知识构建符合物理规则的模拟器，然后使用这些模拟器训练机器人，增强模型的鲁棒性和泛化能力。此外，人工构建常识或符合物理规则的知识并将其应用于生成模型或模拟器是一种常见策略（例如，ElastoGen\[350\]、One-2-3-45\[351\]、PLoT\[349\]）。这种方法对模型施加了更符合物理实际的约束，增强了其在生成任务中的可靠性和可解释性。这些约束确保了模型的知识既准确又一致，减少了训练和应用过程中的不确定性。一些方法将人工创建的物理规则与LLMs或MLMs结合起来。通过利用LLMs和MLMs的常识能力，这些方法（例如，Holodeck\[71\]、LEGENT\[352\]、GRUtopia\[353\]）通过自动空间布局优化生成了多样化且语义丰富的场景。这极大地推进了通用具身代理的发展，通过在新颖和多样化的环境中训练它们。
    

B. 数据收集和训练（Data Collection and Training）

对于仿真到现实适应性，高质量数据非常重要。传统的数据收集方法涉及昂贵的设备、精确的操作，并且耗时、劳动密集，通常缺乏灵活性。最近，一些高效且成本效益高的方法被提出用于高质量演示数据收集和训练。本节将讨论真实世界和模拟环境中数据收集的各种方法。图16展示了真实世界和模拟环境中的演示数据。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

1.  真实世界数据（Real-World Data）：在大量、丰富的数据集上训练大型、高容量模型已经显示出显著的能力和显著的成功，有效地解决了下游应用问题。例如，像ChatGPT、GPT-4和LLaMA这样的LLMs不仅在NLP领域表现出色，而且为下游任务提供了出色的解决问题能力。因此，是否有可能在机器人领域训练一个具身大型模型，通过训练拥有强大的泛化能力，并能够适应新场景和机器人任务。这需要大量的具身数据集为模型训练提供数据。Open X-Embodiment\[303\]是一个来自22种不同机器人的具身数据集，包含527种技能和160,266个任务。收集的数据由机器人执行操作的真实演示数据组成。该数据集主要关注家庭和厨房环境，涉及家具、食物和餐具等物品。操作主要围绕拿起放置任务，一小部分涉及更复杂的操作。在这个数据集上训练的高容量模型RT-X展示了出色的迁移能力。UMI\[358\]提出了一个数据收集和策略学习框架。他们设计了一个手持夹持器和优雅的界面进行数据收集，实现了便携、低成本、信息丰富的数据收集，用于挑战性的双手和动态演示数据。通过简单地修改训练数据，机器人可以实现零样本泛化的双手、精确任务。Mobile ALOHA\[359\]是一个低成本的全身移动操作系统。它可以用于收集全身移动下的双手操作任务数据，如炒虾和上菜。使用这个系统和静态ALOHA收集的数据训练代理可以提高移动操作任务的性能。这样的代理可以作为家庭助手或工作助手。在人类-代理协作\[360\]中，人类和代理在数据收集过程中一起学习，减少人类工作量，加快数据采集速度，提高数据质量。具体来说，在具身场景中，在数据收集期间，人类提供初始动作输入。随后，代理通过迭代微调和去噪过程优化这些动作，逐步产生精确、高质量的操作演示。整个过程可以总结如下：人类在操作中贡献直觉和多样性，而代理处理优化和稳定性，减少对操作员的依赖，使执行更复杂的任务，收集更高质量的数据。
    
2.  模拟数据（Simulated Data）：前述的数据收集方法涉及直接在真实世界中收集演示数据以训练代理。这种收集方法通常需要大量的人力、物力资源和时间，导致效率低下。因此，在大多数情况下，研究人员可以选择在模拟环境中收集数据集进行模型训练。在模拟环境中收集数据不需要大量资源，通常可以由程序自动化，节省大量时间。CLIPORT\[294\]和Transporter Networks\[361\]从Pybullet模拟器收集演示数据，用于端到端网络模型训练，并成功地将模型从模拟转移到真实世界。GAPartNet\[362\]构建了一个以零件为中心的大规模交互数据集GAPartNet，为感知和交互任务提供了丰富的零件级注释。他们提出了一个用于领域泛化的3D零件分割和姿态估计的流程，可以很好地泛化到模拟器和真实世界中未见过的物体类别。SemGrasp\[289\]构建了一个大规模的抓取文本对齐数据集CapGrasp，这是一个来自虚拟环境的语义丰富的灵巧手抓取数据集。
    
3.  仿真到现实范式（Sim-to-Real Paradigms）：最近，引入了几个仿真到现实范式，通过在模拟环境中进行广泛的学习，然后迁移到现实世界设置，以减少对广泛且昂贵的真实世界演示数据的需求。本节概述了五种仿真到现实转移的范式，如图17所示。Real2Sim2real\[363\]通过在“数字孪生”模拟环境中利用强化学习（RL）增强了真实世界场景中的模仿学习。该方法涉及在模拟中通过广泛的RL加强策略，然后将这些策略转移到真实世界以解决数据稀缺问题，并实现有效的机器人操作模仿学习。最初，使用NeRF和VR进行场景扫描和重建，并将构建的场景资产导入模拟器以实现真实到模拟的保真度。随后，在模拟中进行RL以微调从真实世界收集的稀疏专家演示得出的初始策略。最后，将经过改进的策略转移到真实世界设置中。TRANSIC\[364\]通过实时人类干预来纠正真实世界场景中的机器人行为，缩小了仿真到现实的差距。它通过几个步骤增强了仿真到现实的转移性能：首先，机器人在模拟环境中使用RL训练以建立基础策略。然后，这些策略在真实机器人上实施，人类通过远程控制实时干预和纠正错误行为。从这些干预中收集的数据用于训练残差策略。整合基础和残差策略确保了在仿真到现实转移后，真实世界应用中的轨迹更平滑。这种方法显著减少了对真实世界数据收集的需求，从而减轻了负担，同时实现了有效的仿真到现实转移。Domain Randomization\[365\]–\[367\]通过在模拟期间引入参数随机化，增强了在模拟环境中训练的模型对真实世界场景的泛化，涵盖了可能在真实世界设置中发生的条件。这种方法提高了训练模型的鲁棒性，使其能够从模拟环境部署到真实环境。System Identification\[368\]，\[369\]构建了真实世界环境中物理场景的准确数学模型，包括动态和视觉渲染等参数。它的目标是使模拟环境与真实世界设置非常相似，从而促进在模拟中训练的模型顺利过渡到真实环境。Lang4sim2real\[370\]使用自然语言作为桥接，通过使用图像的文本描述作为跨域统一信号来解决仿真到现实的差距。这种方法有助于学习领域不变的图像表示，从而提高跨模拟和真实环境的泛化性能。最初，一个编码器在带有跨域语言描述的图像数据上进行预训练。随后，使用领域不变表示，训练了一个多领域、多任务的语言条件行为克隆策略。这种方法通过从丰富的模拟数据中获取额外信息来补偿真实世界数据的稀缺性，从而增强了仿真到现实转移。
    

C. 具身控制（Embodied Control）

具身控制通过与环境的交互学习，并使用奖励机制优化行为以获得最优策略，从而避免了传统物理建模方法的缺点。具身控制方法可以分为两类：1) 深度强化学习（DRL）。DRL可以处理高维数据并学习复杂的行为模式，使其适合于决策和控制。混合和动态策略梯度（HDPG）\[371\]被提出用于双足运动，允许控制策略根据多个标准动态同时优化。DeepGait \[372\]是一个神经网络策略，用于地形感知运动，它结合了基于模型的运动规划和强化学习的方法。它包括一个地形感知规划器，用于生成步态序列和基础运动，引导机器人朝目标方向前进，以及一个步态和基础运动控制器，用于在保持平衡的同时执行这些序列。规划器和控制器都使用神经网络函数逼近器进行参数化，并使用深度强化学习算法进行优化。2) 模仿学习。DRL的一个缺点是需要大量数据来自众多试验。为了解决这个问题，引入了模仿学习，旨在通过收集高质量的演示来最小化数据使用。为了提高数据效率，提出了Offline RL + Online RL来降低交互成本并确保安全。首先使用离线RL从静态的、预先收集的大型数据集中学习策略。然后将这些策略部署在真实环境中进行实时交互和探索，并根据反馈进行调整。人类演示的代表性模仿学习方法是ALOHA \[373\]和Mobile ALOHA \[359\]。尽管具身AI包括高级算法、模型和规划模块，但其最基础和最关键的组成部分是具身控制。因此，必须考虑如何控制物理实体并赋予它们物理智能。具身控制与硬件密切相关，例如控制关节运动、末端执行器位置和行走速度。对于机器人臂，了解末端执行器的位置，如何规划关节轨迹以将手臂移动到目标？对于仿人机器人，了解运动模式，如何控制关节以实现目标姿势？这些是需要解决的关键问题。一些工作集中在机器人控制上，增强了机器人动作的灵活性。\[374\]提出了一个基于视觉的全身控制框架。通过连接一个机器人臂和一个机器狗，利用所有自由度（腿上有12个关节，臂上有6个关节，夹持器上有1个），它跟踪机器狗的速度和机器人臂的末端执行器位置，实现了更灵活的控制。一些工作\[375\]，\[376\]采用传统方法控制双足机器人行走。MIT的Cheetah 3 \[377\]、ANYmal \[378\]和Atlas \[379\]使用了稳健的行走控制器来管理机器人。这些机器人可以用于更敏捷的运动任务，如跳跃或克服各种障碍\[380\]–\[384\]。其他工作\[385\]，\[386\]专注于仿人机器人的控制，以执行各种动作，模仿人类行为。图18展示了一些例子。具身控制整合了RL和仿真到现实技术，通过环境交互优化策略，使未知领域的探索成为可能，可能超越人类能力，并适应非结构化环境。虽然机器人可以模仿许多人类行为，但有效的任务完成通常需要基于环境反馈的RL训练。最具挑战性的场景包括接触密集型任务，其中操纵需要根据反馈进行实时调整，例如被操纵对象的状态、变形、材料和力量。在这种情况下，RL是不可或缺的。在MLM时代，这些模型具有对场景语义的泛化理解，为RL提供了强大的奖励函数。此外，RL对于将大型模型与预期任务对齐至关重要。未来，在预训练和微调之后，仍然需要RL来与物理世界对齐，确保在真实世界环境中有效部署。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

VIII. 挑战和未来方向

尽管具身AI取得了快速进展，但它面临几个挑战，并提出了激动人心的未来方向。高质量机器人数据集：获取足够的真实世界机器人数据仍然是一个重大挑战。收集这些数据既耗时又耗费资源。仅依赖模拟数据会加剧仿真到现实的差距问题。创建多样化的真实世界机器人数据集需要各种机构之间密切而广泛的合作。此外，开发更现实和高效的模拟器对于提高模拟数据的质量至关重要。当前的工作RT-1 \[11\]使用基于机器人图像和自然语言命令的预训练模型。RT-1在导航和抓取任务中取得了良好的结果，但获取真实世界机器人数据集非常具有挑战性。为了构建能够在机器人学中跨场景和跨任务应用的通用具身模型，必须构建大规模数据集，利用高质量的模拟环境数据协助真实世界数据。高效利用人类演示数据：高效利用人类演示数据涉及利用人类执行的动作和行为来训练和改进机器人系统。这个过程包括收集、处理和从人类执行机器人预期学习的任务的大型、高质量的数据集中学习。当前的工作R3M \[387\]使用动作标签和人类演示数据学习具有高成功率的通用表示，但复杂任务的效率仍需提高。因此，重要的是有效利用大量未结构化、多标签和多模态的人类演示数据，结合动作标签数据，训练能够在短时间内学习各种任务的具身模型。通过高效利用人类演示数据，机器人系统可以实现更高的性能和适应性，使它们更有能力在动态环境中执行复杂任务。复杂环境的认知：复杂环境的认知指的是具身智能体在物理或虚拟环境中感知、理解和导航复杂真实世界环境的能力。基于广泛的常识知识，Say-Can \[299\]利用预训练的LLM模型的任务分解机制，该机制严重依赖大量常识知识进行简单任务规划，但缺乏对复杂环境中长期任务的理解。对于非结构化开放环境，当前的工作通常依赖于使用广泛常识知识的预训练LLM模型的任务分解机制进行简单任务规划，而缺乏特定场景理解。增强知识转移和泛化能力在复杂环境中至关重要。一个真正多功能的机器人系统应该能够理解和执行跨越多样化和未见场景的自然语言指令。这需要开发适应性强、可扩展的具身智能体架构。长期任务执行：执行单个指令通常意味着机器人需要执行长期任务，例如命令“清洁厨房”，这涉及重新排列物体、扫地、擦桌子等活动。成功完成这些任务需要机器人能够计划并在较长时间内执行一系列低级动作。虽然当前的高级任务规划器已经显示出初步的成功，但它们在多样化的场景中往往因为缺乏针对具身任务的调整而不足。解决这一挑战需要开发配备有强大感知能力和丰富常识知识的高效规划器。因果关系发现：现有的数据驱动具身智能体基于数据内在的相关性做出决策。然而，这种建模方法不允许模型真正理解知识、行为和环境之间的因果关系，导致策略存在偏见。这使得确保它们能够以可解释、鲁棒和可靠的方式在真实世界环境中操作变得困难。因此，重要的是让具身智能体由世界知识驱动，能够进行自主因果推理。通过交互和学习理解世界，并通过推理进一步增强多模态具身智能体在复杂真实世界环境中的适应性、决策可靠性和泛化能力。对于具身任务，需要通过交互指令和状态预测建立跨模态的时空因果关系\[388\]。此外，智能体需要理解对象的可承受性，以实现适应性任务规划和动态场景中的长期自主导航。为了优化决策，需要结合反事实和因果干预策略\[389\]，从反事实和因果干预的角度追踪因果关系，减少探索迭代，并优化决策。基于世界知识构建因果图，并通过主动因果推理驱动仿真到现实转移，将为具身AI形成一个统一框架。

持续学习： 在机器人应用中，持续学习\[390\]对于在多样化环境中部署机器人学习策略至关重要，但目前这一领域尚未充分探索。尽管一些近期研究已经考察了持续学习的子主题——如增量学习、快速运动适应和人在环路学习——但这些解决方案通常为单一任务或平台设计，并未考虑基础模型。开放的研究问题和可行的方法包括：1) 在微调最新数据时混合不同比例的先前数据分布，以减轻灾难性遗忘\[391\]；2) 开发从先前分布或课程中高效原型，用于学习新任务时的任务推理；3) 提高在线学习算法的训练稳定性和样本效率；4) 确定将大容量模型无缝整合到控制框架中的原则方法，可能是通过分层学习或慢速-快速控制，以实现实时推理。

统一评估基准： 尽管存在许多评估低级控制策略的基准，但它们通常在评估的技能方面有显著差异。此外，这些基准中包含的对象和场景通常受到模拟器限制。为了全面评估具身模型，需要基准能够使用现实模拟器涵盖一系列多样化的技能。关于高级任务规划器，许多基准侧重于通过问答任务评估规划能力。然而，更理想的方法包括评估高级任务规划器和低级控制策略一起执行长期任务，并测量成功率，而不是仅依赖于规划器的孤立评估。这种综合方法为评估具身AI系统的能力提供了更全面的视角。

IX. 结论

具身AI允许智能体感知、感知并通过网络空间和物理世界与各种对象互动，这对其实现通用人工智能（AGI）至关重要。本调查广泛回顾了具身机器人、模拟器、四个代表性的具身任务：视觉主动感知、具身交互、具身智能体和仿真到现实机器人控制，以及未来的研究方向。对具身机器人、模拟器、数据集和方法的比较总结为最近在具身AI领域的发展提供了清晰的图景，这将极大地有利于未来沿着这一新兴且有前景的研究方向进行的研究。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)作者：张长旺，图源：旺知识

参考资料

标题：Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI

作者：Yang Liu, Weixing Chen, Yongjie Bai, Guanbin Li, Wen Gao, Fellow, IEEE, Liang Lin, Fellow, IEEE

单位：中山大学计算机科学与工程学院，鹏城实验室，北京大学数字媒体研究所

标签：具身人工智能，多模态大型模型，世界模型，智能体，机器人学

概述：本调查研究了具身AI的最新进展，探讨了多模态大型模型和世界模型在实现具身智能体中的重要作用。

链接：https://arxiv.org/pdf/2407.06886v6

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)
